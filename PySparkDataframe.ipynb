{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07710b32",
   "metadata": {},
   "source": [
    "Importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79debfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, ArrayType, FloatType, DateType\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93864bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import findspark\n",
    "findspark.init()\n",
    "sc = SparkSession.builder.appName('Spark').getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b9c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataName = Row('name', 'age', 'country', 'city')\n",
    "\n",
    "data = [dataName('David', '22', 'London', 'Paris'), dataName('Steve', '22', 'New York', 'Sydney')]\n",
    "\n",
    "df = sc.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225e85c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o143.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (Naveenkumar executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df\u001b[39m.\u001b[39;49mshow(\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\rknav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a bool\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    607\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rknav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\rknav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\rknav\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (Naveenkumar executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba6be4",
   "metadata": {},
   "source": [
    "Initiating the findspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd08989e-24cd-432b-8e77-d0d188acd5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba31c2",
   "metadata": {},
   "source": [
    "Creating the spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79bb464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SparkDataFrame\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8add191",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155c1d8",
   "metadata": {},
   "source": [
    "Creating the list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfd5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"James\",\"\", \"Smith\", \"36636\", \"M\", 3000),\n",
    "       (\"Machael\",\"Rose\", \"\", \"40288\", \"M\", 4000),\n",
    "       (\"Robert\",\"\", \"Williams\", \"42114\", \"F\", 4000),\n",
    "       (\"Maria\",\"Anne\", \"Jones\", \"39194\", \"F\", 4000),\n",
    "       (\"Jen\",\"Mary\", \"Brown\", \"\", \"F\", -1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de81d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546984a",
   "metadata": {},
   "source": [
    "Creating the schema to assign the column datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2ecfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"FirstName\", StringType(), True),\n",
    "                   StructField(\"MiddleName\", StringType(), True),\n",
    "                   StructField(\"LastName\", StringType(), True),\n",
    "                   StructField(\"ID\", StringType(), True),\n",
    "                   StructField(\"Gender\", StringType(), True),\n",
    "                   StructField(\"Salary\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d09a8b",
   "metadata": {},
   "source": [
    "Creating the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67684afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f97e6",
   "metadata": {},
   "source": [
    "Print the table schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95fdf389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- MiddleName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9da389",
   "metadata": {},
   "source": [
    "Show the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197ae495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|FirstName|MiddleName|LastName|   ID|Gender|Salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Machael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     F|  4000|\n",
      "|    Maria|      Anne|   Jones|39194|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09a60a",
   "metadata": {},
   "source": [
    "Read CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77361803",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.read.csv(\"employees_data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a5c60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EMPLOYEE_ID: integer (nullable = true)\n",
      " |-- FIRST_NAME: string (nullable = true)\n",
      " |-- LAST_NAME: string (nullable = true)\n",
      " |-- EMAIL: string (nullable = true)\n",
      " |-- PHONE_NUMBER: string (nullable = true)\n",
      " |-- HIRE_DATE: timestamp (nullable = true)\n",
      " |-- JOB_ID: string (nullable = true)\n",
      " |-- SALARY: double (nullable = true)\n",
      " |-- COMMISSION_PCT: double (nullable = true)\n",
      " |-- MANAGER_ID: integer (nullable = true)\n",
      " |-- DEPARTMENT_ID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d068012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+--------+------------+-------------------+----------+-------+--------------+----------+-------------+\n",
      "|EMPLOYEE_ID| FIRST_NAME| LAST_NAME|   EMAIL|PHONE_NUMBER|          HIRE_DATE|    JOB_ID| SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+-----------+----------+--------+------------+-------------------+----------+-------+--------------+----------+-------------+\n",
      "|        100|     Steven|      King|   SKING|515.123.4567|1987-06-17 00:00:00|   AD_PRES|24000.0|           0.0|         0|           90|\n",
      "|        101|      Neena|   Kochhar|NKOCHHAR|515.123.4568|1987-06-18 00:00:00|     AD_VP|17000.0|           0.0|       100|           90|\n",
      "|        102|        Lex|   De Haan| LDEHAAN|515.123.4569|1987-06-19 00:00:00|     AD_VP|17000.0|           0.0|       100|           90|\n",
      "|        103|  Alexander|    Hunold| AHUNOLD|590.423.4567|1987-06-20 00:00:00|   IT_PROG| 9000.0|           0.0|       102|           60|\n",
      "|        104|      Bruce|     Ernst|  BERNST|590.423.4568|1987-06-21 00:00:00|   IT_PROG| 6000.0|           0.0|       103|           60|\n",
      "|        105|      David|    Austin| DAUSTIN|590.423.4569|1987-06-22 00:00:00|   IT_PROG| 4800.0|           0.0|       103|           60|\n",
      "|        106|      Valli| Pataballa|VPATABAL|590.423.4560|1987-06-23 00:00:00|   IT_PROG| 4800.0|           0.0|       103|           60|\n",
      "|        107|      Diana|   Lorentz|DLORENTZ|590.423.5567|1987-06-24 00:00:00|   IT_PROG| 4200.0|           0.0|       103|           60|\n",
      "|        108|      Nancy| Greenberg|NGREENBE|515.124.4569|1987-06-25 00:00:00|    FI_MGR|12000.0|           0.0|       101|          100|\n",
      "|        109|     Daniel|    Faviet| DFAVIET|515.124.4169|1987-06-26 00:00:00|FI_ACCOUNT| 9000.0|           0.0|       108|          100|\n",
      "|        110|       John|      Chen|   JCHEN|515.124.4269|1987-06-27 00:00:00|FI_ACCOUNT| 8200.0|           0.0|       108|          100|\n",
      "|        111|     Ismael|   Sciarra|ISCIARRA|515.124.4369|1987-06-28 00:00:00|FI_ACCOUNT| 7700.0|           0.0|       108|          100|\n",
      "|        112|Jose Manuel|     Urman| JMURMAN|515.124.4469|1987-06-29 00:00:00|FI_ACCOUNT| 7800.0|           0.0|       108|          100|\n",
      "|        113|       Luis|      Popp|   LPOPP|515.124.4567|1987-06-30 00:00:00|FI_ACCOUNT| 6900.0|           0.0|       108|          100|\n",
      "|        114|        Den|  Raphaely|DRAPHEAL|515.127.4561|1987-07-01 00:00:00|    PU_MAN|11000.0|           0.0|       100|           30|\n",
      "|        115|  Alexander|      Khoo|   AKHOO|515.127.4562|1987-07-02 00:00:00|  PU_CLERK| 3100.0|           0.0|       114|           30|\n",
      "|        116|     Shelli|     Baida|  SBAIDA|515.127.4563|1987-07-03 00:00:00|  PU_CLERK| 2900.0|           0.0|       114|           30|\n",
      "|        117|      Sigal|    Tobias| STOBIAS|515.127.4564|1987-07-04 00:00:00|  PU_CLERK| 2800.0|           0.0|       114|           30|\n",
      "|        118|        Guy|    Himuro| GHIMURO|515.127.4565|1987-07-05 00:00:00|  PU_CLERK| 2600.0|           0.0|       114|           30|\n",
      "|        119|      Karen|Colmenares|KCOLMENA|515.127.4566|1987-07-06 00:00:00|  PU_CLERK| 2500.0|           0.0|       114|           30|\n",
      "+-----------+-----------+----------+--------+------------+-------------------+----------+-------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556e709",
   "metadata": {},
   "source": [
    "list the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17bfbfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYEE_ID',\n",
       " 'FIRST_NAME',\n",
       " 'LAST_NAME',\n",
       " 'EMAIL',\n",
       " 'PHONE_NUMBER',\n",
       " 'HIRE_DATE',\n",
       " 'JOB_ID',\n",
       " 'SALARY',\n",
       " 'COMMISSION_PCT',\n",
       " 'MANAGER_ID',\n",
       " 'DEPARTMENT_ID']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a7d06",
   "metadata": {},
   "source": [
    "select the column from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f75e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+\n",
      "|EMPLOYEE_ID| FIRST_NAME|PHONE_NUMBER|\n",
      "+-----------+-----------+------------+\n",
      "|        100|     Steven|515.123.4567|\n",
      "|        101|      Neena|515.123.4568|\n",
      "|        102|        Lex|515.123.4569|\n",
      "|        103|  Alexander|590.423.4567|\n",
      "|        104|      Bruce|590.423.4568|\n",
      "|        105|      David|590.423.4569|\n",
      "|        106|      Valli|590.423.4560|\n",
      "|        107|      Diana|590.423.5567|\n",
      "|        108|      Nancy|515.124.4569|\n",
      "|        109|     Daniel|515.124.4169|\n",
      "|        110|       John|515.124.4269|\n",
      "|        111|     Ismael|515.124.4369|\n",
      "|        112|Jose Manuel|515.124.4469|\n",
      "|        113|       Luis|515.124.4567|\n",
      "|        114|        Den|515.127.4561|\n",
      "|        115|  Alexander|515.127.4562|\n",
      "|        116|     Shelli|515.127.4563|\n",
      "|        117|      Sigal|515.127.4564|\n",
      "|        118|        Guy|515.127.4565|\n",
      "|        119|      Karen|515.127.4566|\n",
      "+-----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file.select(\"EMPLOYEE_ID\", \"FIRST_NAME\", \"PHONE_NUMBER\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0e602a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_1 = r\"C:\\Users\\rknav\\Downloads\\PySpark\\Data\\salesdata\"\n",
    "\n",
    "schema = StructType([StructField(\"Order ID\", StringType(), True),\n",
    "                    StructField(\"Product\", StringType(), True),\n",
    "                    StructField(\"Quantity Ordered\", StringType(), True),\n",
    "                    StructField(\"Price Each\", StringType(), True),\n",
    "                    StructField(\"Order Date\", StringType(), True),\n",
    "                    StructField(\"Purchase Address\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12815f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = (spark.read.format(\"csv\").option(\"header\", True).schema(schema).load(csv_path_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "537290a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a42af171",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_schema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                    StructField(\"first_name\", StringType(), True),\n",
    "                    StructField(\"last_name\", StringType(), True),\n",
    "                    StructField(\"fav_movies\", ArrayType(StringType()), True),\n",
    "                    StructField(\"salary\", FloatType(), True),\n",
    "                    StructField(\"date_of_birth\", DateType(), True),\n",
    "                    StructField(\"image_url\", StringType(), True),\n",
    "                    StructField(\"active\", BooleanType(), True),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a90cf757",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = \"./Data/persons.json\"\n",
    "\n",
    "persion_df = spark.read.json(json_path, person_schema, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0cdc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- fav_movies: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- date_of_birth: date (nullable = true)\n",
      " |-- image_url: string (nullable = true)\n",
      " |-- active: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538fcdb",
   "metadata": {},
   "source": [
    "show data with truncate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d22a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+-------------------------------------------------------------+-------+-------------+-----------------------------------------------+------+\n",
      "|id |first_name|last_name|fav_movies                                                   |salary |date_of_birth|image_url                                      |active|\n",
      "+---+----------+---------+-------------------------------------------------------------+-------+-------------+-----------------------------------------------+------+\n",
      "|1  |Drucy     |Poppy    |[I giorni contati]                                           |1463.36|1991-02-16   |http://dummyimage.com/126x166.png/cc0000/ffffff|true  |\n",
      "|2  |Emelyne   |Blaza    |[Musketeer, The, Topralli]                                   |3006.04|1991-11-02   |http://dummyimage.com/158x106.bmp/cc0000/ffffff|false |\n",
      "|3  |Max       |Rettie   |[The Forgotten Space, Make It Happen]                        |1422.88|1990-03-03   |http://dummyimage.com/237x140.jpg/ff4444/ffffff|false |\n",
      "|4  |Ilario    |Kean     |[Up Close and Personal]                                      |3561.36|1987-06-09   |http://dummyimage.com/207x121.jpg/cc0000/ffffff|true  |\n",
      "|5  |Toddy     |Drexel   |[Walk in the Clouds, A]                                      |4934.87|1992-10-28   |http://dummyimage.com/116x202.png/cc0000/ffffff|true  |\n",
      "|6  |Oswald    |Petrolli |[Wing and the Thigh, The (L'aile ou la cuisse)]              |1153.23|1986-09-02   |http://dummyimage.com/137x172.jpg/5fa2dd/ffffff|false |\n",
      "|7  |Adrian    |Clarey   |[Walking Tall, Paradise, Hawaiian Style]                     |1044.73|1971-08-24   |http://dummyimage.com/244x218.bmp/cc0000/ffffff|false |\n",
      "|8  |Dominica  |Goodnow  |[Hearts Divided]                                             |1147.76|1973-08-27   |http://dummyimage.com/112x203.jpg/dddddd/000000|false |\n",
      "|9  |Emory     |Slocomb  |[Snake and Crane Arts of Shaolin (She hao ba bu), Mala Noche]|1082.11|1974-06-08   |http://dummyimage.com/138x226.jpg/cc0000/ffffff|true  |\n",
      "|10 |Jeremias  |Bode     |[Farewell to Arms, A]                                        |3472.63|1997-08-02   |http://dummyimage.com/243x108.bmp/dddddd/000000|true  |\n",
      "+---+----------+---------+-------------------------------------------------------------+-------+-------------+-----------------------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd0e37",
   "metadata": {},
   "source": [
    "Improting the required functions from sql to perform actions and transfromation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ab202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, concat_ws, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfd7539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------+\n",
      "|first_name|  last_name| salary|\n",
      "+----------+-----------+-------+\n",
      "|     Drucy|      Poppy|1463.36|\n",
      "|   Emelyne|      Blaza|3006.04|\n",
      "|       Max|     Rettie|1422.88|\n",
      "|    Ilario|       Kean|3561.36|\n",
      "|     Toddy|     Drexel|4934.87|\n",
      "|    Oswald|   Petrolli|1153.23|\n",
      "|    Adrian|     Clarey|1044.73|\n",
      "|  Dominica|    Goodnow|1147.76|\n",
      "|     Emory|    Slocomb|1082.11|\n",
      "|  Jeremias|       Bode|3472.63|\n",
      "|   Timothy|     Ervine|1147.61|\n",
      "|   Leanora|     Gooder|1327.02|\n",
      "|  Claiborn|     Denham|2623.33|\n",
      "|   Ambrosi|   Vidineev|4550.88|\n",
      "|    Feodor|Nancekivell|2218.46|\n",
      "|   Margaux|   Archbold|1013.75|\n",
      "|   Balduin|    Elstone|2302.26|\n",
      "|     Alfie|   Hatliffe| 3893.1|\n",
      "|      Lura|     Follis|3331.26|\n",
      "|      Maxi|      Cluet|4046.46|\n",
      "+----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.select(col(\"first_name\"), col(\"last_name\"), col(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ab5a204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|         Full Name| Salary|\n",
      "+------------------+-------+\n",
      "|       Drucy Poppy| 1609.7|\n",
      "|     Emelyne Blaza|3306.64|\n",
      "|        Max Rettie|1565.17|\n",
      "|       Ilario Kean| 3917.5|\n",
      "|      Toddy Drexel|5428.36|\n",
      "|   Oswald Petrolli|1268.55|\n",
      "|     Adrian Clarey| 1149.2|\n",
      "|  Dominica Goodnow|1262.54|\n",
      "|     Emory Slocomb|1190.32|\n",
      "|     Jeremias Bode|3819.89|\n",
      "|    Timothy Ervine|1262.37|\n",
      "|    Leanora Gooder|1459.72|\n",
      "|   Claiborn Denham|2885.66|\n",
      "|  Ambrosi Vidineev|5005.97|\n",
      "|Feodor Nancekivell|2440.31|\n",
      "|  Margaux Archbold|1115.13|\n",
      "|   Balduin Elstone|2532.49|\n",
      "|    Alfie Hatliffe|4282.41|\n",
      "|       Lura Follis|3664.39|\n",
      "|        Maxi Cluet|4451.11|\n",
      "+------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.select(concat_ws(\" \", col(\"first_name\"), col(\"last_name\")).alias(\"Full Name\"), round(expr(\"salary * 0.10 + salary\"), 2).alias(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8217b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "093a2cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+\n",
      "| id|first_name|  last_name|          fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+\n",
      "|  2|   Emelyne|      Blaza|[Musketeer, The, ...|3006.04|   1991-11-02|http://dummyimage...| false|\n",
      "|  4|    Ilario|       Kean|[Up Close and Per...|3561.36|   1987-06-09|http://dummyimage...|  true|\n",
      "|  5|     Toddy|     Drexel|[Walk in the Clou...|4934.87|   1992-10-28|http://dummyimage...|  true|\n",
      "| 10|  Jeremias|       Bode|[Farewell to Arms...|3472.63|   1997-08-02|http://dummyimage...|  true|\n",
      "| 14|   Ambrosi|   Vidineev|[Wall Street: Mon...|4550.88|   1989-07-20|http://dummyimage...|  true|\n",
      "| 18|     Alfie|   Hatliffe|     [Lord of Tears]| 3893.1|   1989-06-21|http://dummyimage...|  true|\n",
      "| 19|      Lura|     Follis|[My Life in Pink ...|3331.26|   1998-11-03|http://dummyimage...| false|\n",
      "| 20|      Maxi|      Cluet|[All I Want for C...|4046.46|   1979-05-06|http://dummyimage...| false|\n",
      "| 21|      Dian|      Dancy|[Double, Double, ...| 3720.3|   1998-12-01|http://dummyimage...|  true|\n",
      "| 22|  Theodore|   Climance|[Story of the Wee...|3008.56|   1999-01-30|http://dummyimage...| false|\n",
      "| 24|  Camellia|   Jervoise| [Waiting to Exhale]|3431.17|   1996-07-14|http://dummyimage...| false|\n",
      "| 25|     Kelcy|     Wogdon|    [Iron Mask, The]|4512.51|   2000-10-20|http://dummyimage...|  true|\n",
      "| 27|    Kelila|Harrowsmith|   [Apparition, The]|4651.58|   1973-01-02|http://dummyimage...|  true|\n",
      "| 29|       Eli|  Normabell|[Return to Peyton...|4917.48|   1993-01-02|http://dummyimage...|  true|\n",
      "| 35|     Fanni|      Dyson|     [Wild One, The]|3228.58|   1995-01-09|http://dummyimage...|  true|\n",
      "| 36|     Ozzie|   Brownlie|     [Orange County]|3945.18|   1992-01-25|http://dummyimage...| false|\n",
      "| 38|    Camile|       Mace|[Family Guy Prese...|3559.93|   1994-12-12|http://dummyimage...| false|\n",
      "| 39|   Melinda|   McKevitt|[Safrana or Freed...|4166.13|   1979-12-15|http://dummyimage...|  true|\n",
      "| 42| Cristiano|       Shaw|[Can-Can, Feeding...|3180.08|   1997-04-07|http://dummyimage...|  true|\n",
      "| 43|   Guthrie|     Veeler|[Mr. Brooks, Seco...|4806.88|   1978-07-31|http://dummyimage...|  true|\n",
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.filter(col(\"salary\") > 3000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8aea65af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+\n",
      "| id|first_name|  last_name|          fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+\n",
      "|  2|   Emelyne|      Blaza|[Musketeer, The, ...|3006.04|   1991-11-02|http://dummyimage...| false|\n",
      "|  4|    Ilario|       Kean|[Up Close and Per...|3561.36|   1987-06-09|http://dummyimage...|  true|\n",
      "|  5|     Toddy|     Drexel|[Walk in the Clou...|4934.87|   1992-10-28|http://dummyimage...|  true|\n",
      "| 10|  Jeremias|       Bode|[Farewell to Arms...|3472.63|   1997-08-02|http://dummyimage...|  true|\n",
      "| 14|   Ambrosi|   Vidineev|[Wall Street: Mon...|4550.88|   1989-07-20|http://dummyimage...|  true|\n",
      "| 18|     Alfie|   Hatliffe|     [Lord of Tears]| 3893.1|   1989-06-21|http://dummyimage...|  true|\n",
      "| 19|      Lura|     Follis|[My Life in Pink ...|3331.26|   1998-11-03|http://dummyimage...| false|\n",
      "| 20|      Maxi|      Cluet|[All I Want for C...|4046.46|   1979-05-06|http://dummyimage...| false|\n",
      "| 21|      Dian|      Dancy|[Double, Double, ...| 3720.3|   1998-12-01|http://dummyimage...|  true|\n",
      "| 22|  Theodore|   Climance|[Story of the Wee...|3008.56|   1999-01-30|http://dummyimage...| false|\n",
      "| 24|  Camellia|   Jervoise| [Waiting to Exhale]|3431.17|   1996-07-14|http://dummyimage...| false|\n",
      "| 25|     Kelcy|     Wogdon|    [Iron Mask, The]|4512.51|   2000-10-20|http://dummyimage...|  true|\n",
      "| 27|    Kelila|Harrowsmith|   [Apparition, The]|4651.58|   1973-01-02|http://dummyimage...|  true|\n",
      "| 29|       Eli|  Normabell|[Return to Peyton...|4917.48|   1993-01-02|http://dummyimage...|  true|\n",
      "| 35|     Fanni|      Dyson|     [Wild One, The]|3228.58|   1995-01-09|http://dummyimage...|  true|\n",
      "| 36|     Ozzie|   Brownlie|     [Orange County]|3945.18|   1992-01-25|http://dummyimage...| false|\n",
      "| 38|    Camile|       Mace|[Family Guy Prese...|3559.93|   1994-12-12|http://dummyimage...| false|\n",
      "| 39|   Melinda|   McKevitt|[Safrana or Freed...|4166.13|   1979-12-15|http://dummyimage...|  true|\n",
      "| 42| Cristiano|       Shaw|[Can-Can, Feeding...|3180.08|   1997-04-07|http://dummyimage...|  true|\n",
      "| 43|   Guthrie|     Veeler|[Mr. Brooks, Seco...|4806.88|   1978-07-31|http://dummyimage...|  true|\n",
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.where(col(\"salary\") > 3000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b7e3af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----------------+-------+-------------+--------------------+------+\n",
      "| id|first_name|last_name|      fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+---------+----------------+-------+-------------+--------------------+------+\n",
      "| 25|     Kelcy|   Wogdon|[Iron Mask, The]|4512.51|   2000-10-20|http://dummyimage...|  true|\n",
      "+---+----------+---------+----------------+-------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.filter(persion_df.first_name == \"Kelcy\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66450e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "| id|first_name|last_name|          fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "|  2|   Emelyne|    Blaza|[Musketeer, The, ...|3006.04|   1991-11-02|http://dummyimage...| false|\n",
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.where(array_contains(persion_df.fav_movies, \"Musketeer, The\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4de2187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "| id|first_name|last_name|          fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "| 37|    Carlen|  Sharply|[Dr. Jekyll and M...|2051.85|   2002-06-01|http://dummyimage...|  true|\n",
      "| 80|   Lorilee|   Petrie|[Gaudi Afternoon,...|4153.15|   2002-07-29|http://dummyimage...| false|\n",
      "| 92|     Daron|  Briance|[Train on the Bra...|4226.35|   2002-02-22|http://dummyimage...|  true|\n",
      "|100|    Virgie| Domanski|[Horseman, The, S...|2165.93|   2002-01-05|http://dummyimage...|  true|\n",
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.where((year(\"date_of_birth\") == 2002) | (year(\"date_of_birth\") == 1888)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aeb39b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "| id|first_name|last_name|          fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "| 37|    Carlen|  Sharply|[Dr. Jekyll and M...|2051.85|   2002-06-01|http://dummyimage...|  true|\n",
      "| 80|   Lorilee|   Petrie|[Gaudi Afternoon,...|4153.15|   2002-07-29|http://dummyimage...| false|\n",
      "| 92|     Daron|  Briance|[Train on the Bra...|4226.35|   2002-02-22|http://dummyimage...|  true|\n",
      "|100|    Virgie| Domanski|[Horseman, The, S...|2165.93|   2002-01-05|http://dummyimage...|  true|\n",
      "+---+----------+---------+--------------------+-------+-------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.where(year(\"date_of_birth\") == 2002).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ef387",
   "metadata": {},
   "source": [
    "Count the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09bfa1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persion_df.select(\"active\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e83a43",
   "metadata": {},
   "source": [
    "Show distinct and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5f7817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|active|\n",
      "+------+\n",
      "|  true|\n",
      "| false|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persion_df.select(\"active\").distinct().show()\n",
    "persion_df.select(\"active\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132a8e7",
   "metadata": {},
   "source": [
    "Order the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19d139fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|first_name|year|active|\n",
      "+----------+----+------+\n",
      "|       Sky|1971| false|\n",
      "|   Feodora|1971|  true|\n",
      "|    Adrian|1971| false|\n",
      "|   Timothy|1971| false|\n",
      "|  Sherline|1972|  true|\n",
      "|     Toddy|1972|  true|\n",
      "|    Lucita|1972|  true|\n",
      "|      Rodi|1972| false|\n",
      "|  Dominica|1973| false|\n",
      "|  Wolfgang|1973|  true|\n",
      "|    Kelila|1973|  true|\n",
      "|     Emory|1974|  true|\n",
      "|   Balduin|1974| false|\n",
      "|    Norean|1974|  true|\n",
      "|    Janean|1975|  true|\n",
      "| Franciska|1976| false|\n",
      "|       Bev|1976|  true|\n",
      "|     Johny|1977| false|\n",
      "|    Bennie|1977| false|\n",
      "|   Guthrie|1978|  true|\n",
      "+----------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.select(col(\"first_name\"), year(col(\"date_of_birth\")).alias(\"year\"), col(\"active\")).orderBy(\"year\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03063e18",
   "metadata": {},
   "source": [
    "Drop duplicate records and order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "032f34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_rec = persion_df.select(col(\"first_name\"), year(col(\"date_of_birth\")).alias(\"year\"), col(\"active\")).drop_duplicates([\"year\", \"active\"]).orderBy(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f150eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|first_name|year|active|\n",
      "+----------+----+------+\n",
      "|   Lorilee|2002| false|\n",
      "|    Carlen|2002|  true|\n",
      "|    Maxine|2001| false|\n",
      "|    Feodor|2000| false|\n",
      "|     Kelcy|2000|  true|\n",
      "| Kendricks|1999|  true|\n",
      "|  Theodore|1999| false|\n",
      "|      Dian|1998|  true|\n",
      "|      Lura|1998| false|\n",
      "|    Kayley|1997| false|\n",
      "|  Jeremias|1997|  true|\n",
      "|  Claiborn|1996| false|\n",
      "|    Giffie|1995| false|\n",
      "|     Fanni|1995|  true|\n",
      "|   Frankie|1994| false|\n",
      "|       Eli|1993|  true|\n",
      "|     Toddy|1992|  true|\n",
      "|     Ozzie|1992| false|\n",
      "|   Emelyne|1991| false|\n",
      "|     Drucy|1991|  true|\n",
      "+----------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dropped_rec.orderBy(\"year\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1f717",
   "metadata": {},
   "source": [
    "Sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8afbf017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------------------+-------+-------------+--------------------+------+\n",
      "| id|first_name| last_name|          fav_movies| salary|date_of_birth|           image_url|active|\n",
      "+---+----------+----------+--------------------+-------+-------------+--------------------+------+\n",
      "| 59|      Wynn|     Sayre|[Misrables in Co...|2689.51|   1997-06-30|http://dummyimage...| false|\n",
      "| 44|  Wolfgang|     Inger|     [No Such Thing]|3192.85|   1973-11-16|http://dummyimage...|  true|\n",
      "| 62|    Wilden|    Mayger|[Rock, Paper, Sci...|3372.25|   1998-12-27|http://dummyimage...| false|\n",
      "| 86|    Welbie|   Crackel|[All That Heaven ...|2720.28|   1990-10-24|http://dummyimage...|  true|\n",
      "|100|    Virgie|  Domanski|[Horseman, The, S...|2165.93|   2002-01-05|http://dummyimage...|  true|\n",
      "| 76|     Trace|     Balke|[Dark Truth, A (T...|4812.73|   1982-01-03|http://dummyimage...|  true|\n",
      "|  5|     Toddy|    Drexel|[Walk in the Clou...|4934.87|   1992-10-28|http://dummyimage...|  true|\n",
      "| 63|     Toddy|Matevosian| [Snake Eyes, Jauja]|4394.08|   1972-03-17|http://dummyimage...|  true|\n",
      "| 89|      Tish|    Machon|  [Arrowhead, Simon]|4830.06|   1995-06-08|http://dummyimage...|  true|\n",
      "| 11|   Timothy|    Ervine|[Land of the Lost...|1147.61|   1971-06-02|http://dummyimage...| false|\n",
      "| 52|  Thorvald|    Finnan|[Star Maker, The ...|2194.92|   1984-09-16|http://dummyimage...| false|\n",
      "| 22|  Theodore|  Climance|[Story of the Wee...|3008.56|   1999-01-30|http://dummyimage...| false|\n",
      "| 91|   Stanley| Sargeaunt|[Blow Out, Ashes,...|2958.64|   1986-09-12|http://dummyimage...| false|\n",
      "| 41|       Sky|     Hails|[Trotsky, The, Su...|1633.95|   1971-02-19|http://dummyimage...| false|\n",
      "| 33|  Sherline|   Primett|   [Jungle Fighters]|2309.39|   1972-07-23|http://dummyimage...|  true|\n",
      "| 61|    Shanna|   Samples|[Thomas in Love (...| 2703.0|   1989-07-07|http://dummyimage...| false|\n",
      "| 66|      Rudy|   Ritelli|[Pope's Toilet, T...|4268.39|   1998-09-01|http://dummyimage...|  true|\n",
      "| 99|   Rozalie|    Wannop|[Suddenly, The No...|1259.64|   1997-03-25|http://dummyimage...| false|\n",
      "| 97|      Rodi|    Farnan|[Code, The (Menta...|2325.88|   1972-01-04|http://dummyimage...| false|\n",
      "| 84|     Ricca|  Newgrosh|[Documentarian, D...|3412.06|   1984-07-27|http://dummyimage...| false|\n",
      "+---+----------+----------+--------------------+-------+-------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persion_df.sort(\"first_name\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7afb42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19aa880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Person=Row(\"name\",\"lang\",\"state\")\n",
    "data = [Person(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \n",
    "    Person(\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"),\n",
    "    Person(\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19c4d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14039fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|            name|              lang|state|\n",
      "+----------------+------------------+-----+\n",
      "|    James,,Smith|[Java, Scala, C++]|   CA|\n",
      "|   Michael,Rose,|[Spark, Java, C++]|   NJ|\n",
      "|Robert,,Williams|      [CSharp, VB]|   NV|\n",
      "+----------------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fa54c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_df = persion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37615a6",
   "metadata": {},
   "source": [
    "Adding the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d70cce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons_df_1 = persons_df.withColumn(\"salary_inc\", round(expr(\"salary * 1.10\"),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da2f3d8",
   "metadata": {},
   "source": [
    "Rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79c843d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, first_name: string, last_name: string, fav_movies: array<string>, salary: float, date_of_birth: date, image_url: string, active: boolean, salary_increment: double]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons_df_1.withColumnRenamed(\"salary_inc\", \"salary_increment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e64ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+----------+\n",
      "| id|first_name|  last_name|          fav_movies| salary|date_of_birth|           image_url|active|salary_inc|\n",
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+----------+\n",
      "|  1|     Drucy|      Poppy|  [I giorni contati]|1463.36|   1991-02-16|http://dummyimage...|  true|    1609.7|\n",
      "|  2|   Emelyne|      Blaza|[Musketeer, The, ...|3006.04|   1991-11-02|http://dummyimage...| false|   3306.64|\n",
      "|  3|       Max|     Rettie|[The Forgotten Sp...|1422.88|   1990-03-03|http://dummyimage...| false|   1565.17|\n",
      "|  4|    Ilario|       Kean|[Up Close and Per...|3561.36|   1987-06-09|http://dummyimage...|  true|    3917.5|\n",
      "|  5|     Toddy|     Drexel|[Walk in the Clou...|4934.87|   1992-10-28|http://dummyimage...|  true|   5428.36|\n",
      "|  6|    Oswald|   Petrolli|[Wing and the Thi...|1153.23|   1986-09-02|http://dummyimage...| false|   1268.55|\n",
      "|  7|    Adrian|     Clarey|[Walking Tall, Pa...|1044.73|   1971-08-24|http://dummyimage...| false|    1149.2|\n",
      "|  8|  Dominica|    Goodnow|    [Hearts Divided]|1147.76|   1973-08-27|http://dummyimage...| false|   1262.54|\n",
      "|  9|     Emory|    Slocomb|[Snake and Crane ...|1082.11|   1974-06-08|http://dummyimage...|  true|   1190.32|\n",
      "| 10|  Jeremias|       Bode|[Farewell to Arms...|3472.63|   1997-08-02|http://dummyimage...|  true|   3819.89|\n",
      "| 11|   Timothy|     Ervine|[Land of the Lost...|1147.61|   1971-06-02|http://dummyimage...| false|   1262.37|\n",
      "| 12|   Leanora|     Gooder|[It's All About L...|1327.02|   1981-12-17|http://dummyimage...| false|   1459.72|\n",
      "| 13|  Claiborn|     Denham|[McCullin, Max Pa...|2623.33|   1996-03-07|http://dummyimage...| false|   2885.66|\n",
      "| 14|   Ambrosi|   Vidineev|[Wall Street: Mon...|4550.88|   1989-07-20|http://dummyimage...|  true|   5005.97|\n",
      "| 15|    Feodor|Nancekivell|   [Monsoon Wedding]|2218.46|   2000-10-07|http://dummyimage...| false|   2440.31|\n",
      "| 16|   Margaux|   Archbold|[And Now a Word f...|1013.75|   1988-07-29|http://dummyimage...|  true|   1115.13|\n",
      "| 17|   Balduin|    Elstone|      [Sisters, The]|2302.26|   1974-07-20|http://dummyimage...| false|   2532.49|\n",
      "| 18|     Alfie|   Hatliffe|     [Lord of Tears]| 3893.1|   1989-06-21|http://dummyimage...|  true|   4282.41|\n",
      "| 19|      Lura|     Follis|[My Life in Pink ...|3331.26|   1998-11-03|http://dummyimage...| false|   3664.39|\n",
      "| 20|      Maxi|      Cluet|[All I Want for C...|4046.46|   1979-05-06|http://dummyimage...| false|   4451.11|\n",
      "+---+----------+-----------+--------------------+-------+-------------+--------------------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "persons_df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pq = r\"C:\\Users\\rknav\\Downloads\\PySpark\\Data\\Generated\"\n",
    "persons_df_1.write.mode(\"overwrite\").parquet(path_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de5bbfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_fs = r\"C:\\Users\\rknav\\Downloads\\PySpark\\Data\\flight-summary.csv\"\n",
    "\n",
    "fs_df = spark.read.csv(path_fs, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cc9a2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['origin_code',\n",
       " 'origin_airport',\n",
       " 'origin_city',\n",
       " 'origin_state',\n",
       " 'dest_code',\n",
       " 'dest_airport',\n",
       " 'dest_city',\n",
       " 'dest_state',\n",
       " 'count']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e07a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd5b3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------------------------------------------------------------+\n",
      "|origin_city|collect_list(dest_state)                                                                    |\n",
      "+-----------+--------------------------------------------------------------------------------------------+\n",
      "|Aberdeen   |[MN]                                                                                        |\n",
      "|Abilene    |[TX]                                                                                        |\n",
      "|Adak       |[AK]                                                                                        |\n",
      "|Agana      |[HI]                                                                                        |\n",
      "|Aguadilla  |[NJ, NY, FL, FL]                                                                            |\n",
      "|Akron      |[NY, NV, FL, MA, FL, CO, MI, NJ, VA, IL, FL, GA]                                            |\n",
      "|Albany     |[GA, NJ, MN, NC, FL, FL, VA, MI, CO, FL, IL, NV, FL, IL, MD, GA]                            |\n",
      "|Albuquerque|[IL, FL, TX, TX, TX, UT, CA, CO, CA, MD, MN, NY, MO, OR, WA, NV, NC, CA, IL, AZ, CA, GA, TX]|\n",
      "|Alexandria |[TX, GA, TX]                                                                                |\n",
      "|Allentown  |[GA, IL, MI]                                                                                |\n",
      "+-----------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fs_df.groupBy(\"origin_city\").agg(collect_list(col(\"dest_state\"))).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e28c1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sc\u001b[39m.\u001b[39mstop()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "76d2056e86dc13fe0446354118b4e59f918722fd0999c16c7f850b6de00a1498"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
